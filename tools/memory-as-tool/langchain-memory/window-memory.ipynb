{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1RvyxOsktOm+6RW0GM8nX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DJOoKBZKXfkk","executionInfo":{"status":"ok","timestamp":1742757070452,"user_tz":-60,"elapsed":10098,"user":{"displayName":"Jaime GÃ³mez Moraleda","userId":"01073662164120725208"}},"outputId":"ecdb2b96-7d7e-48b5-b213-53c5b5b5caa4"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-1-d570ed24850e>:16: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n","  memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n"]},{"output_type":"stream","name":"stdout","text":[" Hello Alice, how can I assist you?\n"," Your name is Alice.\n"," You said \"Hello, I'm Alice.\"\n"," You said \"Your name is Alice.\"\n","{'history': [HumanMessage(content='What was the first thing I said?', additional_kwargs={}, response_metadata={}), AIMessage(content=' You said \"Hello, I\\'m Alice.\"', additional_kwargs={}, response_metadata={}), HumanMessage(content='What was the second thing I said?', additional_kwargs={}, response_metadata={}), AIMessage(content=' You said \"Your name is Alice.\"', additional_kwargs={}, response_metadata={})]}\n"]}],"source":["!pip install -q langchain-openai langchain\n","\n","from google.colab import userdata\n","from langchain_openai import OpenAI\n","from langchain.memory import ConversationBufferWindowMemory\n","from langchain.prompts import PromptTemplate\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain_core.runnables import RunnableLambda\n","from langchain.schema import HumanMessage, AIMessage\n","\n","# 1. API key retrieval and LLM initialisation\n","OPENAI_API_KEY = userdata.get(\"OpenAI-key\")\n","llm = OpenAI(api_key=OPENAI_API_KEY)\n","\n","# 2. Memory initialisation\n","memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n","\n","# 3. Prompt template\n","template = \"\"\"\n","You are a helpful assistant. Use the provided conversation history to answer the user's questions.\n","{history}\n","Human: {input}\n","AI:\"\"\"\n","prompt = PromptTemplate(input_variables=[\"history\", \"input\"], template=template)\n","\n","# 4. Memory loading and saving functions\n","def memory_load(input_data):\n","    return memory.load_memory_variables(input_data)[\"history\"]\n","\n","def add_message(input_output):\n","    memory.save_context({\"input\": input_output[\"input\"]}, {\"output\": input_output[\"output\"]})\n","    return input_output[\"output\"]\n","\n","# 5. Chain construction (RunnableSequence)\n","chain = (\n","    {\"input\": RunnablePassthrough(), \"history\": RunnableLambda(memory_load)}\n","    | prompt\n","    | llm\n","    | RunnableLambda(lambda x: {\"input\": input_data[\"input\"], \"output\":x})\n","    | RunnableLambda(add_message)\n",")\n","\n","# 6. Example conversation\n","input_data = {\"input\": \"Hello, I'm Alice.\"}\n","result1 = chain.invoke(input_data)\n","print(result1)\n","\n","input_data = {\"input\": \"What is my name?\"}\n","result2 = chain.invoke(input_data)\n","print(result2)\n","\n","input_data = {\"input\": \"What was the first thing I said?\"}\n","result3 = chain.invoke(input_data)\n","print(result3)\n","\n","input_data = {\"input\": \"What was the second thing I said?\"}\n","result4 = chain.invoke(input_data)\n","print(result4)\n","\n","# 7. Memory inspection\n","print(memory.load_memory_variables({}))"]}]}