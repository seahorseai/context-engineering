{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8xB+Zh/ZMfPFQxnqvamV0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"AnyG3jaj_4Up","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748357898177,"user_tz":-120,"elapsed":7166,"user":{"displayName":"Jaime Gómez Moraleda","userId":"01073662164120725208"}},"outputId":"f4032bc1-030c-40ef-b48e-5a5355ec36a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.24)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.11.0)\n","Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.82.0)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.18)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n","Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.5.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.61)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n","Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.51.3)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.15.3)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.31.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.2.1)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n"]}],"source":["!pip install -U langchain langchain-community faiss-cpu openai langchain-openai sentence_transformers pypdf"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":39},"id":"YL9W0AjJLfEH","executionInfo":{"status":"ok","timestamp":1748357987150,"user_tz":-120,"elapsed":9281,"user":{"displayName":"Jaime Gómez Moraleda","userId":"01073662164120725208"}},"outputId":"c365547d-06b1-4767-d3db-28c5edbb1458"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-0cd83cd9-f67b-4850-a8c3-30db6e8ae398\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-0cd83cd9-f67b-4850-a8c3-30db6e8ae398\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}}]},{"cell_type":"code","source":["import os\n","print(os.path.exists(\"attention-is-all-you-need.pdf\"))"],"metadata":{"id":"R1fc68M3LhsU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748357977874,"user_tz":-120,"elapsed":12,"user":{"displayName":"Jaime Gómez Moraleda","userId":"01073662164120725208"}},"outputId":"544867cd-3a1d-44ff-e7d5-db0bc8983558"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n"]}]},{"cell_type":"code","source":["from langchain_community.document_loaders import PyPDFLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from itertools import chain\n","from google.colab import userdata\n","from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n","from langchain.vectorstores import FAISS\n","from huggingface_hub import login\n","from sentence_transformers import CrossEncoder\n","from langchain_core.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","\n","#Parent-child chunking\n","parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n","child_splitter = RecursiveCharacterTextSplitter(chunk_size=512)\n","\n","parent_docs = parent_splitter.split_documents(docs)\n","child_docs_nested = [child_splitter.split_documents([doc]) for doc in parent_docs]\n","\n","def parent_child_chunking(child_docs_nested):\n","  child_docs = list(chain.from_iterable(child_docs_nested))\n","  return child_docs\n","\n","child_docs = parent_child_chunking(child_docs_nested)\n","\n","\n","# Rewriting Query\n","def rewrite_query(query, llm):\n","    template = \"\"\"\n","    You are a helpful assistant that rewrites user queries to improve document retrieval.\n","    Consider the following user query and generate a more effective query for a search engine.\n","    The goal is to broaden or clarify the search to find more relevant information.\n","\n","    Original query: {query}\n","\n","    Rewritten query:\n","    \"\"\"\n","    prompt = ChatPromptTemplate.from_template(template)\n","    chain = prompt | llm | StrOutputParser() # Use StrOutputParser to get plain string\n","    rewritten_query = chain.invoke({\"query\": query})\n","    print(f\"Original Query: {query}\")\n","    print(f\"Rewritten Query: {rewritten_query}\")\n","    return rewritten_query\n","\n","\n","\n","# Hybrid Search\n","OPENAI_API_KEY = userdata.get(\"OpenAI-key\")\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.3, openai_api_key=OPENAI_API_KEY)\n","embedder = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n","\n","child_vectorstore = FAISS.from_documents(child_docs, embedder)\n","\n","def hybrid_search(query , child_vectorstore, k_retrieve):\n","    vector_results = child_vectorstore.similarity_search(query, k=k_retrieve)\n","    keyword_results = child_vectorstore.max_marginal_relevance_search(query, k=k_retrieve)\n","    # Combine and deduplicate results\n","    combined = {}\n","    for doc in vector_results + keyword_results:\n","        combined[doc.page_content] = doc\n","    return list(combined.values())\n","\n","# Reranking\n","HF_API_KEY = userdata.get(\"HFAPI-key\")\n","login(HF_API_KEY)\n","cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n","\n","def rerank(query, docs):\n","    scores = cross_encoder.predict([(query, doc.page_content) for doc in docs])\n","    ranked_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n","    return [doc for doc, score in ranked_docs[:5]] # Return top 5 reranked documents\n","\n","# Prompt Engineering\n","def prompt_engineering(reranked, query):\n","  context = \"\\n\".join([f\"Document {i}:: {doc.page_content}\" for i, doc in enumerate(reranked)])\n","  prompt = f\"\"\"Answer using ONLY these documents:\n","{context}\n","Question: {query}\"\"\"\n","  return prompt\n","\n","\n","\n","# Evaluation\n","def evaluate_rag_response(query, retrieved_docs, final_response):\n","    print(\"\\n--- RAG Pipeline Evaluation ---\")\n","    print(f\"Original Query: {query}\")\n","    print(\"\\nRetrieved and Reranked Documents:\")\n","    for i, doc in enumerate(retrieved_docs):\n","        print(f\"Document {i+1}:\")\n","        print(f\"Content: {doc.page_content[:200]}...\") # Print first 200 chars\n","        print(\"-\" * 30)\n","\n","    print(\"\\nFinal Generated Response:\")\n","    print(final_response)\n","\n","    print(\"\\n--- End Evaluation ---\")\n","\n","\n","# RAG Pipeline Function\n","def advanced_rag(query, llm, child_docs_nested, child_vectorstore, k_retrieve=30, k_final=5, enable_query_rewriting=True):\n","\n","    # Parent-child chunking (already done during setup, but keeping this call for conceptual clarity if it were dynamic)\n","    parent_child_chunking(child_docs_nested)\n","\n","    # Rewriting Query\n","    original_query = query # Keep original for evaluation\n","    if enable_query_rewriting:\n","        query = rewrite_query(query, llm)\n","\n","    # Hybrid retrieval\n","    combined_docs = hybrid_search(query, child_vectorstore,  k_retrieve )\n","\n","    # Reranking\n","    reranked = rerank(query, combined_docs)\n","\n","    # Format prompt\n","    prompt = prompt_engineering(reranked, query)\n","\n","    final_response = llm.invoke(prompt).content # Get the string content of the response\n","\n","    # Return everything needed for evaluation\n","    return original_query, reranked, final_response\n","\n","# Example Usage\n","print(\"--- Running RAG with Query Rewriting and Evaluation ---\")\n","query = \"What is the self-attention mechanism in Transformers?\"\n","original_query, retrieved_docs_for_eval, final_response_obj = advanced_rag(query, llm, child_docs_nested, child_vectorstore, enable_query_rewriting=True)\n","\n","# Call the evaluation function\n","evaluate_rag_response(original_query, retrieved_docs_for_eval, final_response_obj)\n","\n","print(\"\\n\\n--- Running RAG without Query Rewriting (for comparison) ---\")\n","query_no_rewrite = \"What is the self-attention mechanism in Transformers?\"\n","original_query_no_rewrite, retrieved_docs_no_rewrite, final_response_no_rewrite = advanced_rag(query_no_rewrite, llm, child_docs_nested, child_vectorstore, enable_query_rewriting=False)\n","\n","# Call the evaluation function for comparison\n","evaluate_rag_response(original_query_no_rewrite, retrieved_docs_no_rewrite, final_response_no_rewrite)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"55CP1MeOm49b","executionInfo":{"status":"ok","timestamp":1748363973224,"user_tz":-120,"elapsed":20586,"user":{"displayName":"Jaime Gómez Moraleda","userId":"01073662164120725208"}},"outputId":"c9673c50-44a5-454f-d2ca-de21aab76bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Running RAG with Query Rewriting and Evaluation ---\n","Original Query: What is the self-attention mechanism in Transformers?\n","Rewritten Query: Explain the concept of self-attention mechanism in Transformers and its significance.\n","\n","--- RAG Pipeline Evaluation ---\n","Original Query: What is the self-attention mechanism in Transformers?\n","\n","Retrieved and Reranked Documents:\n","Document 1:\n","Content: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n","to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n","descr...\n","------------------------------\n","Document 2:\n","Content: typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n","[38, 2, 9].\n","• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n","and quer...\n","------------------------------\n","Document 3:\n","Content: encoder.\n","• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n","all positions in the decoder up to and including that position. We need to prevent leftward\n","i...\n","------------------------------\n","Document 4:\n","Content: Figure 1: The Transformer - model architecture.\n","The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n","connected layers for both the encoder and decoder, ...\n","------------------------------\n","Document 5:\n","Content: Attention mechanisms have become an integral part of compelling sequence modeling and transduc-\n","tion models in various tasks, allowing modeling of dependencies without regard to their distance in\n","the ...\n","------------------------------\n","\n","Final Generated Response:\n","The self-attention mechanism in Transformers allows different positions within a sequence to relate to each other in order to compute a representation of the sequence. This mechanism is used in both the encoder and decoder of the Transformer model architecture. In the encoder, self-attention layers enable each position to attend to all positions in the previous layer, while in the decoder, each position can attend to all positions up to and including that position. This helps in capturing dependencies within the sequence without relying on recurrence, leading to more efficient and effective sequence modeling. The Multi-Head Attention in Transformers helps counteract the reduced effective resolution caused by averaging attention-weighted positions, enhancing the overall performance of the model.\n","\n","--- End Evaluation ---\n","\n","\n","--- Running RAG without Query Rewriting (for comparison) ---\n","\n","--- RAG Pipeline Evaluation ---\n","Original Query: What is the self-attention mechanism in Transformers?\n","\n","Retrieved and Reranked Documents:\n","Document 1:\n","Content: reduced to a constant number of operations, albeit at the cost of reduced effective resolution due\n","to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\n","descr...\n","------------------------------\n","Document 2:\n","Content: Figure 1: The Transformer - model architecture.\n","The Transformer follows this overall architecture using stacked self-attention and point-wise, fully\n","connected layers for both the encoder and decoder, ...\n","------------------------------\n","Document 3:\n","Content: typical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n","[38, 2, 9].\n","• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\n","and quer...\n","------------------------------\n","Document 4:\n","Content: encoder.\n","• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\n","all positions in the decoder up to and including that position. We need to prevent leftward\n","i...\n","------------------------------\n","Document 5:\n","Content: of a single sequence in order to compute a representation of the sequence. Self-attention has been\n","used successfully in a variety of tasks including reading comprehension, abstractive summarization,\n","t...\n","------------------------------\n","\n","Final Generated Response:\n","The self-attention mechanism in Transformers is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence.\n","\n","--- End Evaluation ---\n"]}]}]}