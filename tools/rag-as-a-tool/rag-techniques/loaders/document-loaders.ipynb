{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMC41OcghYX5sy46foItqdu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -q langchain pypdf requests beautifulsoup4 langchain-community\n","\n","import requests\n","from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","# --- PDF Loading and Processing (Local File) ---\n","pdf_file_path = \"academic-paper.pdf\" # Replace with your PDF file name.\n","\n","try:\n","    pdf_loader = PyPDFLoader(pdf_file_path)\n","    pdf_documents = pdf_loader.load()\n","    print(f\"Successfully loaded {len(pdf_documents)} documents from {pdf_file_path}\")\n","except FileNotFoundError:\n","    print(f\"Error: PDF file '{pdf_file_path}' not found. Please upload the file to Google Colab's file explorer.\")\n","    pdf_documents = []\n","except Exception as e:\n","    print(f\"An error occurred during PDF loading: {e}\")\n","    pdf_documents = []\n","\n","pdf_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","pdf_chunks = pdf_text_splitter.split_documents(pdf_documents)\n","if pdf_documents:\n","  print(f\"Split PDF into {len(pdf_chunks)} chunks.\")\n","\n","if pdf_chunks:\n","    print(\"\\nPDF Chunk Metadata Example:\")\n","    print(pdf_chunks[0].metadata)\n","\n","# --- Web Page Loading and Processing (Wikipedia Example) ---\n","wikipedia_url = \"https://en.wikipedia.org/wiki/LangChain\"\n","headers = {\n","    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n","}\n","\n","try:\n","    response = requests.get(wikipedia_url, headers=headers)\n","    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)\n","\n","    web_loader = WebBaseLoader(wikipedia_url, requests_kwargs={'headers': headers})\n","    web_documents = web_loader.load()\n","    print(f\"\\nSuccessfully loaded {len(web_documents)} documents from {wikipedia_url}\")\n","except requests.exceptions.RequestException as e:\n","    print(f\"Error loading web page {wikipedia_url}: {e}\")\n","    web_documents = []\n","except Exception as e:\n","    print(f\"An unexpected error occurred: {e}\")\n","    web_documents = []\n","\n","web_text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","web_chunks = web_text_splitter.split_documents(web_documents)\n","if web_documents:\n","  print(f\"Split Wikipedia content into {len(web_chunks)} chunks.\")\n","\n","if web_chunks:\n","    print(\"\\nWikipedia Chunk Metadata Example:\")\n","    print(web_chunks[0].metadata)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ioQPzHIgXC52","executionInfo":{"status":"ok","timestamp":1743093034600,"user_tz":-60,"elapsed":13958,"user":{"displayName":"Jaime Gómez Moraleda","userId":"01073662164120725208"}},"outputId":"9293cdb4-87b2-4669-b3c4-f7b93406a311"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"]},{"output_type":"stream","name":"stdout","text":["Successfully loaded 7 documents from academic-paper.pdf\n","Split PDF into 32 chunks.\n","\n","PDF Chunk Metadata Example:\n","{'producer': 'Microsoft® Word Microsoft 365 için', 'creator': 'Microsoft® Word Microsoft 365 için', 'creationdate': '2023-07-22T11:45:02+03:00', 'author': 'TuRGuT', 'moddate': '2023-07-22T11:45:02+03:00', 'source': 'academic-paper.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}\n","\n","Successfully loaded 1 documents from https://en.wikipedia.org/wiki/LangChain\n","Split Wikipedia content into 23 chunks.\n","\n","Wikipedia Chunk Metadata Example:\n","{'source': 'https://en.wikipedia.org/wiki/LangChain', 'title': 'LangChain - Wikipedia', 'language': 'en'}\n"]}]}]}